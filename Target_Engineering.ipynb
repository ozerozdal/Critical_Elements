{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import rasterio\n",
    "from GeoDS import hypercube\n",
    "from GeoDS.prospectivity import hyperparameterstuning\n",
    "from GeoDS import utilities\n",
    "from GeoDS.supervised import mapclass\n",
    "from GeoDS.prospectivity import reporting \n",
    "from GeoDS.prospectivity import featureimportance as fe\n",
    "from GeoDS import eda\n",
    "from GeoDS import datawrangle\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "import glob\n",
    "\n",
    "import optuna\n",
    "from optuna import pruners\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import tensorflow_data_validation as tfdv\n",
    "#from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "#print('TFDV Version: {}'.format(tfdv.__version__))\n",
    "#print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "plt.rcParams[\"figure.facecolor\"] = 'white'\n",
    "plt.rcParams[\"axes.facecolor\"] = 'white'\n",
    "plt.rcParams[\"savefig.facecolor\"] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e6e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = 'epsg:26918'\n",
    "AOI = 'Inputs/AOI/shape/AOI_geol.shp'\n",
    "xRes = 5\n",
    "yRes = 5\n",
    "pixel_size = 5\n",
    "\n",
    "# Random seed\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7682db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'Baseline_Model_June21'\n",
    "\n",
    "reporting_folder = os.path.join(trial_name, 'reporting/')\n",
    "output_folder = os.path.join(trial_name, 'outputs/')\n",
    "predictions_folder = os.path.join(trial_name, 'predictions/')\n",
    "    \n",
    "if not os.path.exists(reporting_folder):\n",
    "    os.makedirs(reporting_folder)\n",
    "        \n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "if not os.path.exists(predictions_folder):\n",
    "    os.makedirs(predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6541510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import rioxarray\n",
    "from osgeo import gdal, gdalconst\n",
    "from GeoDS import utilities\n",
    "import subprocess\n",
    "import xarray\n",
    "import json\n",
    "\n",
    "def _reproject_tif_folder(input_folder, output_folder, crs, xRes, yRes):\n",
    "    tifs = glob.glob(os.path.join(input_folder, '*.tif'))\n",
    "\n",
    "    if not len(tifs) > 0:\n",
    "        raise ValueError(\n",
    "            \"The input folder you provided do not contain any geotiffs. Please check your spelling.\")\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for t in tifs:\n",
    "        filename, ext, directory = utilities.Path_Info(t)\n",
    "        newname = filename + '.tif'\n",
    "        newpath = os.path.join(output_folder, newname)\n",
    "\n",
    "        utilities.warp(input_geotiff=t, output_geotiff=newpath,\n",
    "                       dstSRS=crs, xRes=xRes, yRes=yRes)\n",
    "\n",
    "\n",
    "def _get_geotiff_information(input_tif):\n",
    "    src = rasterio.open(input_tif)\n",
    "    if(src.crs == None):\n",
    "        # In case no CRS exists\n",
    "        crs = 'INVALID CRS. CRS = None. PLEASE CHECK.'\n",
    "    else:\n",
    "        try:\n",
    "            # We need a try statement in case the CRS is a weird output by oasis montaj. Could not repeat the exact issue I had with the output from Michael Cain but I think this will handle future issues.\n",
    "            crs = src.crs['init']\n",
    "        except:\n",
    "            crs = 'INVALID CRS. CANNOT ACCES [\\'init\\'] property. PLEASE CHECK.'\n",
    "            pass\n",
    "\n",
    "    shape = src.shape\n",
    "    nb_bands = src.count\n",
    "    types = src.dtypes\n",
    "    nodata_values_by_bands = src.nodatavals\n",
    "    nodata_value = src.nodata\n",
    "    # not using the src.transform\n",
    "    try:\n",
    "        gt = src.transform\n",
    "        pixelSizeX = gt[0]\n",
    "        pixelSizeY = -gt[4]\n",
    "        resolution = (pixelSizeX, pixelSizeY)\n",
    "    except:\n",
    "        resolution = ('error cannot access transform attribute',\n",
    "                      'error cannot access transform attribute')\n",
    "        pass\n",
    "\n",
    "    from shapely.geometry import box\n",
    "    bounds = src.bounds\n",
    "    geom = box(*bounds)\n",
    "    src.close()\n",
    "\n",
    "    return crs, shape, nb_bands, types, nodata_values_by_bands, nodata_value, resolution, geom\n",
    "\n",
    "\n",
    "def sanity_check(input_directories, working_aoi, working_crs, output_directory='Sanity_Report/'):\n",
    "    \"\"\"\n",
    "    Performs a validation on each on the input layers. Will output a csv to assess shape, crs, nb_bands, resolution, bounding box in working AOI, nodata value\n",
    "    I suggest you do this and before each project, make sure that all the layers have the right and same CRS, the same no-data value, are single-bands, non corrupt and contained in the working AOI.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_directory : list\n",
    "        list where each element is a string of the path to the directory which contains all the input geotiff\n",
    "    working_aoi : str\n",
    "        path to a shapefile containing a sole polygon of the aoi. That polygon should have only one field called 'value' and set to 1\n",
    "    working_crs : str\n",
    "        the destination CRS, such as \"epsg:26921\" per example. Not implemented yet, will help to have a column that tells if CRS is good or not.\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    Examples\n",
    "    --------\n",
    "        Perform a sanity check. Will ouput a report (excel format) in Sanity_Report/ folder .\n",
    "        >>> datawrangle.sanity_check(input_directories='path_to_input/layers/folder/', working_aoi='path/to/aoi.shp', working_crs='epsg:31981')\n",
    "    \"\"\"\n",
    "    files_names = []\n",
    "    for dir in input_directories:\n",
    "        names = glob.glob(os.path.join(dir, \"*.tif\"))\n",
    "        files_names.extend(names)\n",
    "\n",
    "    if (len(files_names) == 0):\n",
    "        # This means that the input_directory is empty or no data is contained.\n",
    "        print(\"Input folder is either empty or you gave the wrong path. Please double check and come back.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(files_names, columns=['file_name'])\n",
    "    df['crs'], df['shape'], df['nb_bands'], df['types'], df['nodata_values_by_bands'], df['nodata_value'], df[\n",
    "        'resolution_x_y'], df['bounding_box'] = zip(\n",
    "        *df.apply(lambda x: _get_geotiff_information(x['file_name']), axis=1))\n",
    "\n",
    "    aoi_gdf = gpd.read_file(working_aoi)\n",
    "    aoi_poly = aoi_gdf['geometry'].loc[0]\n",
    "    df['is_in_aoi'] = df.apply(\n",
    "        lambda x: x['bounding_box'].intersects(aoi_poly), axis=1)\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "\n",
    "    # Save bouding boxes\n",
    "    d = {'file_name': df['file_name'], 'geometry': df['bounding_box']}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=working_crs)\n",
    "    gdf.to_file(os.path.join(output_directory, 'Layers_Bouding_boxes.shp'))\n",
    "\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "    final_name = os.path.join(\n",
    "        output_directory, 'Sanity_Report_' + now + '.xlsx')\n",
    "    df.to_excel(final_name)\n",
    "    print(\"Sanity check completed. Please see the report file %s \" % final_name)\n",
    "\n",
    "\n",
    "def make_abstract_grid_csv(aoi_tif, output_directory):\n",
    "    \"\"\"\n",
    "    Outputs a .csv containing x,y coordinates of all the cells of the AOI rasters (an abstract grid).\n",
    "    Parameters\n",
    "    ----------\n",
    "    aoi_tif : str\n",
    "        path to the aoi geotiff\n",
    "    output_directory : str\n",
    "        path to where the abstract grid will be saved\n",
    "    Returns\n",
    "    -------\n",
    "    grid : str\n",
    "        path to the abstract grid csv\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    grid = os.path.join(output_directory, 'abstract_grid.csv')\n",
    "    if os.path.exists(grid):\n",
    "        os.remove(grid)\n",
    "\n",
    "    utilities.geotiff_to_csv(aoi_tif, grid)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def build_stack_cdf(input_numerical_folder, input_categorical_folder, output_folder, aoi_shapefile, working_crs, x_res, y_res, output_format='csv', ensure_tif_reprojection=True):\n",
    "    \"\"\"\n",
    "    DataWrangle main function. Creates a hypercube csv using NetCDF as internal mechanism for enhanced performances.\n",
    "    Outputs a .csv of the HyperCube.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_numerical_folder: str\n",
    "        directory containing numerical layers\n",
    "    input_categorical_folder: str\n",
    "        directory containing categorical layers\n",
    "    output_folder : str\n",
    "        directory for the hypercube output\n",
    "    aoi_shapefile: str\n",
    "        path to a shapefile that has a polygon representing the area of interest\n",
    "    working_crs: str\n",
    "        working CRS, per example 'epsg:26921'\n",
    "    x_res: int\n",
    "        x_resolution\n",
    "    y_res: int\n",
    "        y_resolution\n",
    "    output_format : str, default='csv'\n",
    "        Output format for the cube. Available options are 'csv' or 'netCDF'\n",
    "    ensure_tif_reprojection : bool, default = True\n",
    "        Will automatically perform gdal.warp on each of the individual input files to fix unexpected behaviors when it comes to build a VRT. Performance will be decreased but we suggest to keep it this way.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Examples\n",
    "    -------\n",
    "        Build a HyperCube (csv).\n",
    "        >>> datawrangle.build_stack_cdf(input_numerical_folder='input_layers_numerical/', input_categorical_folder='input_layers_categorical/', output_folder='output/', aoi_shapefile='AOI/AOI_2021.shp', working_crs='epsg:31981', x_res=25, y_res=25)\n",
    "    \"\"\"\n",
    "    warnings.warn(\n",
    "        \"This function is deprecated, Please use cube_vrt if this function throws NetCDF errors\", ResourceWarning)\n",
    "    print(\"Processing...\")\n",
    "    # QAQC the output format\n",
    "    formats = ['csv', 'netcdf']\n",
    "    output_format = output_format.lower()\n",
    "    if output_format not in formats:\n",
    "        raise ValueError(\n",
    "            \"Output format you specified is not supported. Supported types : 'csv', 'netcdf'. Processing aborted.\")\n",
    "\n",
    "    # QAQC THE AOI\n",
    "    if(os.path.isfile(aoi_shapefile)):\n",
    "        aoi = gpd.read_file(aoi_shapefile)\n",
    "        if(len(aoi.geom_type) == 0):\n",
    "            print(\"Your AOI shapefile is empty. Processing aborted.\")\n",
    "            return\n",
    "        else:\n",
    "            for t in aoi.geom_type:\n",
    "                if t != 'Polygon':\n",
    "                    print(\n",
    "                        'There is a geometry that is not a polygon in your AOI. Go in QGIS and fix this. Processing aborted.')\n",
    "                    return\n",
    "\n",
    "            for v in aoi['geometry'].is_valid:\n",
    "                if v == False:\n",
    "                    print(\n",
    "                        'There is an invalid geometry in your polygon (self-crossing itself, per example). Go in QGIS and fix this. Processing aborted.')\n",
    "                    return\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The path of the AOI shapefile you provided is wrong. Please double check. Processing aborted.\")\n",
    "\n",
    "    # QAQC if to see if folder provided do exists and are not empty\n",
    "    numeric_files = glob.glob(os.path.join(input_numerical_folder, \"*.tif\"))\n",
    "    if (len(numeric_files) == 0):\n",
    "        raise ValueError(\n",
    "            \"The input NUMERICAL folder path you provided do not contain any geotiffs. Processing aborted.\")\n",
    "\n",
    "    cat_files = glob.glob(os.path.join(input_categorical_folder, \"*.tif\"))\n",
    "    if (len(cat_files) == 0):\n",
    "        raise ValueError(\n",
    "            \"The input CATEGORICAL folder path you provided do not contain any geotiffs. Processing aborted.\")\n",
    "\n",
    "    if(ensure_tif_reprojection == True):\n",
    "        print(\"Reprojecting your numerical files\")\n",
    "        output_folder_numerical_warped = os.path.join(\n",
    "            output_folder, 'datawrangle_reprojected_numerical/')\n",
    "        _reproject_tif_folder(\n",
    "            input_numerical_folder, output_folder_numerical_warped, working_crs, x_res, y_res)\n",
    "        numeric_files = glob.glob(os.path.join(\n",
    "            output_folder_numerical_warped, '*.tif'))\n",
    "\n",
    "        print(\"Reprojecting your categorical files\")\n",
    "        output_folder_categorical_warped = os.path.join(\n",
    "            output_folder, 'datawrangle_reprojected_categorical/')\n",
    "        _reproject_tif_folder(\n",
    "            input_categorical_folder, output_folder_categorical_warped, working_crs, x_res, y_res)\n",
    "        cat_files = glob.glob(os.path.join(\n",
    "            output_folder_categorical_warped, '*.tif'))\n",
    "\n",
    "    print(\"Cubing numerical files...\")\n",
    "    _to_netcdf(numeric_files, os.path.join('temp_numeric_cube.nc'), working_aoi_polygon=aoi_shapefile, working_crs=working_crs,\n",
    "               x_res=x_res, y_res=y_res, resampling_method='cubic', output_type=gdal.GDT_Unknown)\n",
    "    _rename_bands(numeric_files, os.path.join('temp_numeric_cube.nc'))\n",
    "    print(\"Partial numerical cube on {} layers done\".format(len(numeric_files)))\n",
    "\n",
    "    _to_netcdf(cat_files, os.path.join('temp_cat_cube.nc'), working_aoi_polygon=aoi_shapefile, working_crs=working_crs,\n",
    "               x_res=x_res, y_res=y_res, resampling_method='near', output_type=gdal.GDT_Int16)\n",
    "    _rename_bands(cat_files, os.path.join('temp_cat_cube.nc'))\n",
    "    print(\"Partial categorical cube on {} layers done\".format(len(cat_files)))\n",
    "    print(\"merging categorical and numerical features into one...\")\n",
    "\n",
    "    _merge_cubes(os.path.join('renamed_temp_numeric_cube.nc'), os.path.join(\n",
    "        'renamed_temp_cat_cube.nc'), output_folder, output_format)\n",
    "    print(\"Cube creation completed. Cleaning unnecessary files from the disk\")\n",
    "\n",
    "    temp_ncs = [os.path.join('temp_cat_cube.nc'), os.path.join('temp_numeric_cube.nc'), os.path.join(\n",
    "        'renamed_temp_numeric_cube.nc'), os.path.join('renamed_temp_cat_cube.nc')]\n",
    "    for f in temp_ncs:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_netcdf(input_geotiffs, output_nc, working_aoi_polygon, working_crs, x_res, y_res, resampling_method,\n",
    "               output_type, nodata=-99999):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_type: GDAL outputtype used for experientation\n",
    "    input_geotiffs: isolate dinput directory containing only numerical or categorical dataset\n",
    "    output_nc: Output directory\n",
    "    working_aoi_polygon: working area of interest\n",
    "    working_crs: working CRS\n",
    "    x_res: X resolution\n",
    "    y_res: y resolution\n",
    "    resampling_method : resampling method 'near' for categorical dataset and 'cubic' for numerical\n",
    "    nodata: -99999 represents all the null values in the dataset\n",
    "    Returns\n",
    "    -------\n",
    "    individual processed files  with .nc extension in your selected directory\n",
    "    \"\"\"\n",
    "\n",
    "    buildvrt_options = gdal.BuildVRTOptions(xRes=x_res, yRes=y_res, targetAlignedPixels=True, outputSRS=working_crs,\n",
    "                                            VRTNodata=nodata, resampleAlg=resampling_method, separate=True)\n",
    "    one_vrt = gdal.BuildVRT(\n",
    "        destName=\"\", srcDSOrSrcDSTab=input_geotiffs, options=buildvrt_options)\n",
    "    # We Warp it for alignement, resampling, correct CRS and cropping.\n",
    "    warp_options = gdal.WarpOptions(format='NetCDF', xRes=x_res, yRes=y_res, targetAlignedPixels=True,\n",
    "                                    dstSRS=working_crs,\n",
    "                                    cutlineDSName=working_aoi_polygon, resampleAlg=resampling_method,\n",
    "                                    srcNodata=nodata,\n",
    "                                    dstNodata=nodata,\n",
    "                                    outputType=output_type)\n",
    "    warp_output_vrt = gdal.Warp(\n",
    "        srcDSOrSrcDSTab=one_vrt, destNameOrDestDS=output_nc, options=warp_options)\n",
    "\n",
    "    if(warp_output_vrt == None):\n",
    "        print(\"output of gdal.warp() is None. Possible causes are corrupted input files or inputs files not contained in AOI. Processing aborded.\")\n",
    "\n",
    "        return\n",
    "    warp_output_vrt = None\n",
    "    output_nc = None\n",
    "    return\n",
    "\n",
    "\n",
    "def _rename_bands(input_files, nc_file):\n",
    "    # PH's comment : Jan 28 2022. This function could be removed easily and performances would increase drastically also. It is just a matter of finding how to rename \"dimensions\" in rioxarray which I did not knew the week I built datawrangle.\n",
    "\n",
    "    renames = {}\n",
    "    for index, f in enumerate(input_files):\n",
    "        filename, directory, extension = utilities.Path_Info(f)\n",
    "        band_name = 'Band' + str(index + 1)\n",
    "        renames[band_name] = filename\n",
    "    # rename the output_nc step\n",
    "    ds = xarray.open_dataset(nc_file)\n",
    "    x = ds.rename(renames)\n",
    "\n",
    "    x.to_netcdf(os.path.join('renamed_' + nc_file))\n",
    "    ds.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _merge_cubes(a, b, output_folder, output_format):\n",
    "    print(\"If merging looks to take forever, restart your kernel and delete the temp files.\")\n",
    "    cube = xarray.open_mfdataset([a, b], parallel=True)\n",
    "    cube = cube.drop_vars(['transverse_mercator'])\n",
    "    print('Merging completed')\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if(output_format == 'csv'):\n",
    "        print(\"Writing ouput CSV, this is the longest step, it may take a while...\")\n",
    "        out_name = os.path.join(output_folder, 'Hypercube_' + now + '.csv')\n",
    "        df = cube.to_dataframe().reset_index()\n",
    "        #     df.drop(columns=['transverse_mercator'], inplace=True)\n",
    "        df.set_index(['x', 'y'], inplace=True)\n",
    "        df = df.dropna(axis=0, how='all')\n",
    "        df.to_csv(out_name, index=True, chunksize=10000)\n",
    "    elif(output_format == 'netcdf'):\n",
    "        print(\"Writing output NetCDF file (nc)...\")\n",
    "        out_name = os.path.join(\n",
    "            output_folder, 'Netcdf_Hypercube_' + now + '.nc')\n",
    "        cube.to_netcdf(out_name)\n",
    "\n",
    "    cube.close()\n",
    "    print(\"Cube built with success.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def reconstruct_geotiffs_from_cube(input_cube, output_folder, crs, x_field='x', y_field='y', xRes=25, yRes=25):\n",
    "    \"\"\"\n",
    "    QAQC function to create rasters for each column of the HyperCube. The goal is that both the geologist and data scientist could have a look at the resampled, realigned geotiff and confirm the cube is well in shaped and ML work can start.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_cube : str\n",
    "        path to the input csv of the HyperCube\n",
    "    output_folder : str\n",
    "        directory where to save new output folders\n",
    "    crs : str\n",
    "        crs of the project, example 'epsg:26921'\n",
    "    x_field : str, default='x'\n",
    "        name of the x coordinate column\n",
    "    y_field : str, default='y'\n",
    "        name of the y coordinate column\n",
    "    xRes : int, default=25\n",
    "        x resolution\n",
    "    yRes : int, default=25\n",
    "        y resolution\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Examples\n",
    "    --------\n",
    "        Rasterize each column of the cube back to its original geotiff format.\n",
    "        >>> datawrangle.reconstruct_geotiffs_from_cube(input_cube='Netcdf_Hypercube_10-22-2021_1621.csv', output_folder='QAQC_outputs/', crs='epsg:26921', x_field='x', y_field='y', xRes=25, yRes=25)\n",
    "    \"\"\"\n",
    "    # open the dataframe, read only the columns\n",
    "    df = pd.read_csv(input_cube, nrows=2)\n",
    "    cols = df.columns\n",
    "\n",
    "    # get all columns\n",
    "    for col in cols:\n",
    "        if (col != x_field and col != y_field):\n",
    "            print(\"Rebuilding column \" + col)\n",
    "            new_name = 'Rebuilt_from_cube_' + col + '.tif'\n",
    "            output = output_folder + new_name\n",
    "            # utilities.csv_to_raster(input_cube, output,crs, x_field, y_field, col, xRes, yRes)\n",
    "            utilities.csv_to_raster(\n",
    "                input_cube, output, crs, x_field, y_field, col, xRes, yRes)\n",
    "\n",
    "    print('Cube reconstructed. See in ' + output_folder)\n",
    "    return None\n",
    "\n",
    "def get_bands(file_list):\n",
    "    bands = []\n",
    "    for e in file_list:\n",
    "        filename, ext, directory = utilities.Path_Info(e)\n",
    "        bands.append(filename)\n",
    "    return bands\n",
    "\n",
    "def set_band_descriptions(filepath, bands):\n",
    "    \"\"\"\n",
    "    filepath: path/virtual path/uri to raster\n",
    "    bands:    ((band, description), (band, description),...)\n",
    "    \"\"\"\n",
    "    bands_numbers = np.arange(1, len(bands) + 1, 1).tolist()\n",
    "    bands = zip(bands_numbers, bands)\n",
    "    ds = gdal.Open(filepath, gdal.GA_Update)\n",
    "    for band, desc in bands:\n",
    "        rb = ds.GetRasterBand(band)\n",
    "        rb.SetDescription(desc)\n",
    "    del ds\n",
    "\n",
    "def read_vrt_to_df(vrt):\n",
    "    \"\"\" Read a VRT stack into a pandas.DataFrame with x,y coordinates columns\n",
    "    for compatibility with datawrangle.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vrt : str\n",
    "        Path to VRT file to be read.\n",
    "    \"\"\"\n",
    "    xar = rioxarray.open_rasterio(vrt)\n",
    "        \n",
    "    if type(xar.attrs[\"long_name\"]) == str:\n",
    "        feats = xar.attrs['long_name']\n",
    "        feats = list(xar.attrs['long_name'])\n",
    "        feats = ''.join(feats)\n",
    "        feats = [feats]\n",
    "    if type(xar.attrs[\"long_name\"]) == tuple:\n",
    "        feats = list(xar.attrs['long_name'])\n",
    "            \n",
    "    del xar\n",
    "        \n",
    "    # Read data and flatten arrays\n",
    "    with rioxarray.open_rasterio(vrt, masked=True) as ds:\n",
    "        x_coords = ds.coords['x'].values\n",
    "        y_coords = ds.coords['y'].values\n",
    "        data_array = ds.data\n",
    "        l, m, n = data_array.shape\n",
    "        data_flat = np.empty((m * n, l))\n",
    "        for i in range(l):\n",
    "            flat = data_array[i].flatten()\n",
    "            data_flat[:, i] = flat\n",
    "    # Extract coordinates - this imbricated loop oculd be enhanced\n",
    "    i = 0\n",
    "    coords = np.empty((m * n, 2))\n",
    "    for y in y_coords:\n",
    "        for x in x_coords:\n",
    "            coords[i] = [x, y]\n",
    "            i += 1\n",
    "    # Concatenate and write df\n",
    "    final_array = np.concatenate((coords, data_flat), axis=1)\n",
    "    headers = ['x', 'y'] + feats\n",
    "    del data_flat\n",
    "    df = pd.DataFrame(data=final_array, columns=headers)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cube_vrt(input_folders_list, \n",
    "             output_folder, \n",
    "             x_res, \n",
    "             y_res, \n",
    "             working_crs, \n",
    "             working_aoi_polygon, \n",
    "             save_vrt=True, \n",
    "             save_csv=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_folders_list : list\n",
    "        list of list with all folders containing input data\n",
    "    output_folder : str\n",
    "        Given output folder name will be created containing the cube data.\n",
    "    x_res: int\n",
    "        x resolution\n",
    "    y_res: int\n",
    "        y resolution\n",
    "    working_crs : str\n",
    "        working CRS, per example 'epsg:26921'\n",
    "    working_aoi_polygon: str\n",
    "        path to a shapefile that has a polygon representing the area of interest\n",
    "    save_vrt : Boolean\n",
    "        default : True\n",
    "        Saves vrt format of the cube in output folder\n",
    "    save_csv : Boolean\n",
    "        default : True\n",
    "        Saves csv format of the cube in output folder\n",
    "    Returns\n",
    "    -------\n",
    "    cube : pandas.DataFrame\n",
    "        resultant cube\n",
    "    Examples\n",
    "    --------\n",
    "    # cube\n",
    "    crs = 'epsg:32733'\n",
    "    x_res = 10\n",
    "    y_res = 10\n",
    "    output_folder = 'project_outputs/'\n",
    "    pred_aoi = 'aoi/Prediction AOI.shp'\n",
    "    geofez_layers = 'prediction_features/geofez/'\n",
    "    haralick_layers = 'prediction_features/unidirectional_w_5_d_1/'\n",
    "    stat_layers = 'prediction_features/stat_features_window_kernel_5_5/'\n",
    "    lbp_layers = 'prediction_features/lbp/'\n",
    "    label_layers = 'prediction_features/labels/'\n",
    "    input_directories = [geofez_layers,haralick_layers,stat_layers,lbp_layers,label_layers]\n",
    "    datawrangle.cube_vrt(input_directories, output_folder, x_res, y_res, crs, pred_aoi, \n",
    "    save_vrt=True, save_csv=True)\n",
    "    \"\"\"\n",
    "        \n",
    "    all_files = []\n",
    "    bands = [] \n",
    "    columns = {}\n",
    "    coord = {'coordinates' : ['x', 'y']} \n",
    "    columns.update(coord)\n",
    "    input_name_list = []\n",
    "    \n",
    "    for f in input_folders_list:\n",
    "        \n",
    "        temp = glob.glob(os.path.join(f, \"*.tif\"))\n",
    "        \n",
    "        for j in temp:\n",
    "            input_name = os.path.basename(j).split('.')[0]\n",
    "            input_name_list.append(input_name)\n",
    "        \n",
    "        if(len(temp) == 0):\n",
    "            raise ValueError(\n",
    "                \"The input folder path %f you provided do not contain any geotiffs. Processing aborted.\" % f)\n",
    "        all_files.extend(temp)\n",
    "        bands.extend(get_bands(temp))\n",
    "        features = {'features' : input_name_list}\n",
    "        \n",
    "    columns.update(features)\n",
    "        \n",
    "    parent = Path(input_folders_list[0]).parent\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "\n",
    "    nodata = -9999\n",
    "    #print(\"There are %s numerical layers and %s categorical layers \"% (str(len(numeric_files)), str(len(cat_files))))\n",
    "\n",
    "    buildvrt_options1 = gdal.BuildVRTOptions(xRes=x_res, yRes=y_res, targetAlignedPixels=True, outputSRS=working_crs,\n",
    "                                             VRTNodata=nodata, resampleAlg='near', separate=True)\n",
    "\n",
    "    combined = os.path.join(parent, \"combined_cat_numeric_vrt_\"+now+\".vrt\")\n",
    "    numerical_vrt = gdal.BuildVRT(\n",
    "        destName=combined, srcDSOrSrcDSTab=all_files, options=buildvrt_options1)\n",
    "    numerical_vrt = None\n",
    "\n",
    "    # change band1 band2 band3 by the real variable name\n",
    "    #bands = get_bands(numeric_files) + get_bands(cat_files)\n",
    "    set_band_descriptions(combined, bands)\n",
    "\n",
    "    warp_options = gdal.WarpOptions(format='vrt', xRes=x_res, yRes=y_res, targetAlignedPixels=True,\n",
    "                                    dstSRS=working_crs,\n",
    "                                    cutlineDSName=working_aoi_polygon, resampleAlg='near',\n",
    "                                    cropToCutline=True,\n",
    "                                    outputType=gdal.GDT_Float32,\n",
    "                                    dstNodata=nodata, copyMetadata=True\n",
    "                                    )\n",
    "\n",
    "    output_vrt = os.path.join(parent, 'hypercube_' + now + '.vrt')\n",
    "    warp_output_vrt = gdal.Warp(\n",
    "        srcDSOrSrcDSTab=combined, destNameOrDestDS=output_vrt, options=warp_options)\n",
    "    warp_output_vrt = None\n",
    "\n",
    "    print('Converting vrt to dataframe... may take a while')\n",
    "        \n",
    "    cube = read_vrt_to_df(output_vrt)\n",
    "    \n",
    "    print('Now writing columns in json')\n",
    "    now = utilities.actual_time_for_file_name()    \n",
    "    output_json_name = os.path.join(output_folder, 'columns_' + now + '.json')\n",
    "    with open(os.path.join(output_json_name), 'w') as fp:\n",
    "        json.dump(columns, fp)      \n",
    "       \n",
    "    if(save_csv == True):\n",
    "        print('Now writing csv ....... may take a while')\n",
    "        output_cube_name = os.path.join(output_folder, os.path.basename(output_vrt).split('.')[0] + '.csv')\n",
    "        cube.to_csv(output_cube_name, index=False)\n",
    "    else:\n",
    "        print(\"Not saving the cube to csv.\")\n",
    "              \n",
    "    if(save_vrt == False):\n",
    "        os.remove(combined)\n",
    "        os.remove(output_vrt)\n",
    "    else:\n",
    "        print(\"Your hypercube was also saved under .vrt format (two files necessary) in the input data folder : %s \" % parent)\n",
    "\n",
    "    print(\"Cube Creation Completed. \")\n",
    "    return cube, columns\n",
    "\n",
    "def add_features(\n",
    "    current_dataframe,\n",
    "    input_directories,\n",
    "    output_folder,\n",
    "    x_res,\n",
    "    y_res,\n",
    "    crs,\n",
    "    aoi_shapefile,\n",
    "    save_vrt=False,\n",
    "    save_csv=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Add extra features to existing data cube\n",
    "    Example:\n",
    "\n",
    "    trial_name = 'RawBaseline_Amaruq_May29'\n",
    "    input_directories = ['targets/']\n",
    "    output_folder = os.path.join(trial_name, 'outputs/')\n",
    "    x_res = 25\n",
    "    y_res = 25\n",
    "    crs = 'epsg:26914'\n",
    "    aoi_shapefile_amaruq = 'AOI_Amaruq/Amaruq_AOI.shp'\n",
    "\n",
    "    df_combined, columns = datawrangle.add_features(\n",
    "            df_current,    \n",
    "            input_directories,\n",
    "            output_folder, \n",
    "            x_res, \n",
    "            y_res,\n",
    "            crs, \n",
    "            aoi_shapefile_amaruq, \n",
    "            save_vrt=True,\n",
    "            save_csv=False\n",
    "            )\n",
    "    \"\"\"\n",
    "    df_addition, _ = cube_vrt(\n",
    "        input_directories,\n",
    "        output_folder, \n",
    "        x_res,\n",
    "        y_res,\n",
    "        crs, \n",
    "        aoi_shapefile,\n",
    "        save_vrt=save_vrt, \n",
    "        save_csv=save_csv\n",
    "        )\n",
    "\n",
    "    columns = {}\n",
    "    coord = {'coordinates' : ['x', 'y']} \n",
    "    columns.update(coord)\n",
    "\n",
    "    current_dataframe = pd.merge(current_dataframe, df_addition, on=(columns['coordinates']))\n",
    "\n",
    "    feature_list = current_dataframe.columns[(current_dataframe.columns != 'x') & (current_dataframe.columns != 'y')].to_list()\n",
    "    features = {'features' : feature_list}\n",
    "    columns.update(features)\n",
    "    \n",
    "    print('Now writing columns in json')\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "    output_json_name = os.path.join(output_folder, 'columns_' + now + '.json')\n",
    "    with open(os.path.join(output_json_name), 'w') as fp:\n",
    "        json.dump(columns, fp)  \n",
    "        \n",
    "    if(save_csv == True):\n",
    "        now = utilities.actual_time_for_file_name()\n",
    "        output_cube = os.path.join(output_folder, 'hypercube_' + now + '.csv')\n",
    "        print('Now writing csv ....... may take a while')\n",
    "        current_dataframe.to_csv(output_cube, index=False)\n",
    "        print('Done!')\n",
    "    else:\n",
    "        print(\"Not saving the cube to csv.\")\n",
    "\n",
    "    return current_dataframe, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e438428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting vrt to dataframe... may take a while\n",
      "Now writing columns in json\n",
      "Not saving the cube to csv.\n",
      "Cube Creation Completed. \n"
     ]
    }
   ],
   "source": [
    "input_directories = [\n",
    "                    'Inputs/Targets/Raster/Positives',\n",
    "                    'Inputs/Targets/Raster/Negatives'\n",
    "                    ] \n",
    "\n",
    "df_Input, columns = cube_vrt(input_directories, \n",
    "            output_folder, \n",
    "            xRes, \n",
    "            yRes, \n",
    "            crs, \n",
    "            AOI, \n",
    "            save_vrt=False, \n",
    "            save_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6880fa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>CRE_rock_Li100ppm_10mbuffer_pos</th>\n",
       "      <th>CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos</th>\n",
       "      <th>CRE_trench_Li100ppm_10mbuffer_pos</th>\n",
       "      <th>CRE_rock_Li5ppm_10mbuffer_neg</th>\n",
       "      <th>CRE_trench_Li5ppm_10mbuffer_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384727.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>384732.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384737.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384742.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384747.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358840493</th>\n",
       "      <td>517892.5</td>\n",
       "      <td>5693437.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358840494</th>\n",
       "      <td>517897.5</td>\n",
       "      <td>5693437.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358840495</th>\n",
       "      <td>517902.5</td>\n",
       "      <td>5693437.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358840496</th>\n",
       "      <td>517907.5</td>\n",
       "      <td>5693437.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358840497</th>\n",
       "      <td>517912.5</td>\n",
       "      <td>5693437.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358840498 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x          y  CRE_rock_Li100ppm_10mbuffer_pos  \\\n",
       "0          384727.5  5760787.5                              NaN   \n",
       "1          384732.5  5760787.5                              NaN   \n",
       "2          384737.5  5760787.5                              NaN   \n",
       "3          384742.5  5760787.5                              NaN   \n",
       "4          384747.5  5760787.5                              NaN   \n",
       "...             ...        ...                              ...   \n",
       "358840493  517892.5  5693437.5                              NaN   \n",
       "358840494  517897.5  5693437.5                              NaN   \n",
       "358840495  517902.5  5693437.5                              NaN   \n",
       "358840496  517907.5  5693437.5                              NaN   \n",
       "358840497  517912.5  5693437.5                              NaN   \n",
       "\n",
       "           CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "...                                            ...   \n",
       "358840493                                      NaN   \n",
       "358840494                                      NaN   \n",
       "358840495                                      NaN   \n",
       "358840496                                      NaN   \n",
       "358840497                                      NaN   \n",
       "\n",
       "           CRE_trench_Li100ppm_10mbuffer_pos  CRE_rock_Li5ppm_10mbuffer_neg  \\\n",
       "0                                        NaN                            NaN   \n",
       "1                                        NaN                            NaN   \n",
       "2                                        NaN                            NaN   \n",
       "3                                        NaN                            NaN   \n",
       "4                                        NaN                            NaN   \n",
       "...                                      ...                            ...   \n",
       "358840493                                NaN                            NaN   \n",
       "358840494                                NaN                            NaN   \n",
       "358840495                                NaN                            NaN   \n",
       "358840496                                NaN                            NaN   \n",
       "358840497                                NaN                            NaN   \n",
       "\n",
       "           CRE_trench_Li5ppm_10mbuffer_neg  \n",
       "0                                      NaN  \n",
       "1                                      NaN  \n",
       "2                                      NaN  \n",
       "3                                      NaN  \n",
       "4                                      NaN  \n",
       "...                                    ...  \n",
       "358840493                              NaN  \n",
       "358840494                              NaN  \n",
       "358840495                              NaN  \n",
       "358840496                              NaN  \n",
       "358840497                              NaN  \n",
       "\n",
       "[358840498 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d1d196f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coordinates': ['x', 'y'],\n",
       " 'features': ['CRE_rock_Li100ppm_10mbuffer_pos',\n",
       "  'CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos',\n",
       "  'CRE_trench_Li100ppm_10mbuffer_pos',\n",
       "  'CRE_rock_Li5ppm_10mbuffer_neg',\n",
       "  'CRE_trench_Li5ppm_10mbuffer_neg']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f28d6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     2828, 358837670]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['CRE_rock_Li5ppm_10mbuffer_neg'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c2de528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([       21, 358840477]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['CRE_trench_Li5ppm_10mbuffer_neg'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fd56050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([      153, 358840345]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2af9c9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     1285, 358839213]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['CRE_rock_Li100ppm_10mbuffer_pos'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00261aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([       35, 358840463]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['CRE_trench_Li100ppm_10mbuffer_pos'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7994b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87986bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = np.where(df_Input['CRE_rock_Li5ppm_10mbuffer_neg'] == 1,\n",
    "                   df_Input['CRE_rock_Li5ppm_10mbuffer_neg'],\n",
    "                   df_Input['CRE_trench_Li5ppm_10mbuffer_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f2db784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     2849, 358837649]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new_arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aed6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = np.where(df_Input['CRE_rock_Li100ppm_10mbuffer_pos'] == 1,\n",
    "                   df_Input['CRE_rock_Li100ppm_10mbuffer_pos'],\n",
    "                   df_Input['CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07f8a196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     1409, 358839089]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new_arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be5421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b83eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     2849, 358837649]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input['Combined_neg'] = df_Input['CRE_rock_Li5ppm_10mbuffer_neg'].combine_first(df_Input['CRE_trench_Li5ppm_10mbuffer_neg'])\n",
    "np.unique(df_Input['Combined_neg'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cff7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_Input.loc[df_Input['Combined_neg'] == 1.].index\n",
    "df_Input.loc[indices, 'Combined_neg'] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7e086a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0., nan]), array([     2849, 358837649]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_Input['Combined_neg'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ad0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a624f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     1409, 358839089]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input['Combined_pos'] = df_Input['CRE_rock_Li100ppm_10mbuffer_pos'].combine_first(df_Input['CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos'])\n",
    "np.unique(df_Input['Combined_pos'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff02a08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1., nan]), array([     1422, 358839076]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input['Combined_pos'] = df_Input['Combined_pos'].combine_first(df_Input['CRE_trench_Li100ppm_10mbuffer_pos'])\n",
    "np.unique(df_Input['Combined_pos'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ce374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da161024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1., nan]), array([     2821,      1422, 358836255]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input['Target'] = df_Input['Combined_pos'].combine_first(df_Input['Combined_neg'])\n",
    "np.unique(df_Input['Target'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc30f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dee9242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x', 'y', 'CRE_rock_Li100ppm_10mbuffer_pos',\n",
       "       'CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos',\n",
       "       'CRE_trench_Li100ppm_10mbuffer_pos', 'CRE_rock_Li5ppm_10mbuffer_neg',\n",
       "       'CRE_trench_Li5ppm_10mbuffer_neg', 'Combined_neg', 'Combined_pos',\n",
       "       'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a1fab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384727.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>384732.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384737.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384742.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384747.5</td>\n",
       "      <td>5760787.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y  Target\n",
       "0  384727.5  5760787.5     NaN\n",
       "1  384732.5  5760787.5     NaN\n",
       "2  384737.5  5760787.5     NaN\n",
       "3  384742.5  5760787.5     NaN\n",
       "4  384747.5  5760787.5     NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['CRE_rock_Li100ppm_10mbuffer_pos',\n",
    "       'CRE_DDH_Li100ppm_25mdepth_10mbuffer_pos',\n",
    "       'CRE_trench_Li100ppm_10mbuffer_pos', 'CRE_rock_Li5ppm_10mbuffer_neg',\n",
    "       'CRE_trench_Li5ppm_10mbuffer_neg', 'Combined_neg', 'Combined_pos']\n",
    "\n",
    "df_Input.drop(columns=columns_to_drop, inplace=True)\n",
    "df_Input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426a75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc2ef9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "sd = dd.from_pandas(df_Input, npartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e08af971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Demo/Baseline_Model_June21/outputs/Targets_Merged-00.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-01.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-02.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-03.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-04.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-05.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-06.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-07.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-08.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-09.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-10.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-11.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-12.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-13.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-14.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-15.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-16.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-17.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-18.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-19.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-20.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-21.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-22.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-23.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-24.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-25.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-26.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-27.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-28.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-29.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-30.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-31.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-32.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-33.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-34.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-35.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-36.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-37.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-38.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-39.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-40.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-41.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-42.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-43.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-44.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-45.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-46.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-47.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-48.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-49.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-50.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-51.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-52.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-53.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-54.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-55.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-56.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-57.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-58.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-59.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-60.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-61.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-62.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-63.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-64.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-65.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-66.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-67.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-68.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-69.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-70.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-71.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-72.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-73.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-74.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-75.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-76.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-77.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-78.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-79.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-80.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-81.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-82.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-83.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-84.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-85.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-86.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-87.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-88.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-89.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-90.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-91.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-92.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-93.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-94.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-95.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-96.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-97.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-98.csv',\n",
       " '/Demo/Baseline_Model_June21/outputs/Targets_Merged-99.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.to_csv(os.path.join(output_folder, 'Targets_Merged-*.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804a818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geods Kernel",
   "language": "python",
   "name": "geods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
