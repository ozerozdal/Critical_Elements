{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea5b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import rasterio\n",
    "from GeoDS import hypercube\n",
    "from GeoDS.prospectivity import hyperparameterstuning\n",
    "from GeoDS import utilities\n",
    "from GeoDS.supervised import mapclass\n",
    "from GeoDS.prospectivity import reporting \n",
    "from GeoDS.prospectivity import featureimportance as fe\n",
    "from GeoDS import eda\n",
    "from GeoDS import datawrangle\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "import glob\n",
    "from dask import dataframe as dd\n",
    "\n",
    "import optuna\n",
    "from optuna import pruners\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import tensorflow_data_validation as tfdv\n",
    "#from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "#print('TFDV Version: {}'.format(tfdv.__version__))\n",
    "#print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "plt.rcParams[\"figure.facecolor\"] = 'white'\n",
    "plt.rcParams[\"axes.facecolor\"] = 'white'\n",
    "plt.rcParams[\"savefig.facecolor\"] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efe8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_classes(actual_classes, correspondance_dict):\n",
    "    original_classes = [correspondance_dict[code] for code in actual_classes]\n",
    "    return original_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a443c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = 'epsg:26918'\n",
    "AOI = 'Inputs/AOI/shape/AOI_geol.shp'\n",
    "xRes = 5\n",
    "yRes = 5\n",
    "pixel_size = 5\n",
    "\n",
    "# Random seed\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e7fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'test_folder'\n",
    "\n",
    "reporting_folder = os.path.join(trial_name, 'reporting/')\n",
    "output_folder = os.path.join(trial_name, 'outputs/')\n",
    "predictions_folder = os.path.join(trial_name, 'predictions/')\n",
    "CatBoost_predictions_folder = os.path.join(predictions_folder, 'CatBoost_predictions/')\n",
    "RF_predictions_folder = os.path.join(predictions_folder, 'RF_predictions/')\n",
    "\n",
    "if not os.path.exists(reporting_folder):\n",
    "    os.makedirs(reporting_folder)\n",
    "        \n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "if not os.path.exists(predictions_folder):\n",
    "    os.makedirs(predictions_folder) \n",
    "\n",
    "if not os.path.exists(CatBoost_predictions_folder):\n",
    "    os.makedirs(CatBoost_predictions_folder)\n",
    "\n",
    "if not os.path.exists(RF_predictions_folder):\n",
    "    os.makedirs(RF_predictions_folder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7793919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import rioxarray\n",
    "from osgeo import gdal, gdalconst\n",
    "from GeoDS import utilities\n",
    "import subprocess\n",
    "import xarray\n",
    "import json\n",
    "\n",
    "\n",
    "def _reproject_tif_folder(input_folder, output_folder, crs, xRes, yRes):\n",
    "    tifs = glob.glob(os.path.join(input_folder, '*.tif'))\n",
    "\n",
    "    if not len(tifs) > 0:\n",
    "        raise ValueError(\n",
    "            \"The input folder you provided do not contain any geotiffs. Please check your spelling.\")\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for t in tifs:\n",
    "        filename, ext, directory = utilities.Path_Info(t)\n",
    "        newname = filename + '.tif'\n",
    "        newpath = os.path.join(output_folder, newname)\n",
    "\n",
    "        utilities.warp(input_geotiff=t, output_geotiff=newpath,\n",
    "                       dstSRS=crs, xRes=xRes, yRes=yRes)\n",
    "\n",
    "\n",
    "def _get_geotiff_information(input_tif):\n",
    "    src = rasterio.open(input_tif)\n",
    "    if(src.crs == None):\n",
    "        # In case no CRS exists\n",
    "        crs = 'INVALID CRS. CRS = None. PLEASE CHECK.'\n",
    "    else:\n",
    "        try:\n",
    "            # We need a try statement in case the CRS is a weird output by oasis montaj. Could not repeat the exact issue I had with the output from Michael Cain but I think this will handle future issues.\n",
    "            crs = src.crs['init']\n",
    "        except:\n",
    "            crs = 'INVALID CRS. CANNOT ACCES [\\'init\\'] property. PLEASE CHECK.'\n",
    "            pass\n",
    "\n",
    "    shape = src.shape\n",
    "    nb_bands = src.count\n",
    "    types = src.dtypes\n",
    "    nodata_values_by_bands = src.nodatavals\n",
    "    nodata_value = src.nodata\n",
    "    # not using the src.transform\n",
    "    try:\n",
    "        gt = src.transform\n",
    "        pixelSizeX = gt[0]\n",
    "        pixelSizeY = -gt[4]\n",
    "        resolution = (pixelSizeX, pixelSizeY)\n",
    "    except:\n",
    "        resolution = ('error cannot access transform attribute',\n",
    "                      'error cannot access transform attribute')\n",
    "        pass\n",
    "\n",
    "    from shapely.geometry import box\n",
    "    bounds = src.bounds\n",
    "    geom = box(*bounds)\n",
    "    src.close()\n",
    "\n",
    "    return crs, shape, nb_bands, types, nodata_values_by_bands, nodata_value, resolution, geom\n",
    "\n",
    "\n",
    "def sanity_check(input_directories, working_aoi, working_crs, output_directory='Sanity_Report/'):\n",
    "    \"\"\"\n",
    "    Performs a validation on each on the input layers. Will output a csv to assess shape, crs, nb_bands, resolution, bounding box in working AOI, nodata value\n",
    "    I suggest you do this and before each project, make sure that all the layers have the right and same CRS, the same no-data value, are single-bands, non corrupt and contained in the working AOI.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_directory : list\n",
    "        list where each element is a string of the path to the directory which contains all the input geotiff\n",
    "    working_aoi : str\n",
    "        path to a shapefile containing a sole polygon of the aoi. That polygon should have only one field called 'value' and set to 1\n",
    "    working_crs : str\n",
    "        the destination CRS, such as \"epsg:26921\" per example. Not implemented yet, will help to have a column that tells if CRS is good or not.\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    Examples\n",
    "    --------\n",
    "        Perform a sanity check. Will ouput a report (excel format) in Sanity_Report/ folder .\n",
    "        >>> datawrangle.sanity_check(input_directories='path_to_input/layers/folder/', working_aoi='path/to/aoi.shp', working_crs='epsg:31981')\n",
    "    \"\"\"\n",
    "    files_names = []\n",
    "    for dir in input_directories:\n",
    "        names = glob.glob(os.path.join(dir, \"*.tif\"))\n",
    "        files_names.extend(names)\n",
    "\n",
    "    if (len(files_names) == 0):\n",
    "        # This means that the input_directory is empty or no data is contained.\n",
    "        print(\"Input folder is either empty or you gave the wrong path. Please double check and come back.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(files_names, columns=['file_name'])\n",
    "    df['crs'], df['shape'], df['nb_bands'], df['types'], df['nodata_values_by_bands'], df['nodata_value'], df[\n",
    "        'resolution_x_y'], df['bounding_box'] = zip(\n",
    "        *df.apply(lambda x: _get_geotiff_information(x['file_name']), axis=1))\n",
    "\n",
    "    aoi_gdf = gpd.read_file(working_aoi)\n",
    "    aoi_poly = aoi_gdf['geometry'].loc[0]\n",
    "    df['is_in_aoi'] = df.apply(\n",
    "        lambda x: x['bounding_box'].intersects(aoi_poly), axis=1)\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "\n",
    "    # Save bouding boxes\n",
    "    d = {'file_name': df['file_name'], 'geometry': df['bounding_box']}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=working_crs)\n",
    "    gdf.to_file(os.path.join(output_directory, 'Layers_Bouding_boxes.shp'))\n",
    "\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "    final_name = os.path.join(\n",
    "        output_directory, 'Sanity_Report_' + now + '.xlsx')\n",
    "    df.to_excel(final_name)\n",
    "    print(\"Sanity check completed. Please see the report file %s \" % final_name)\n",
    "\n",
    "\n",
    "def make_abstract_grid_csv(aoi_tif, output_directory):\n",
    "    \"\"\"\n",
    "    Outputs a .csv containing x,y coordinates of all the cells of the AOI rasters (an abstract grid).\n",
    "    Parameters\n",
    "    ----------\n",
    "    aoi_tif : str\n",
    "        path to the aoi geotiff\n",
    "    output_directory : str\n",
    "        path to where the abstract grid will be saved\n",
    "    Returns\n",
    "    -------\n",
    "    grid : str\n",
    "        path to the abstract grid csv\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    grid = os.path.join(output_directory, 'abstract_grid.csv')\n",
    "    if os.path.exists(grid):\n",
    "        os.remove(grid)\n",
    "\n",
    "    utilities.geotiff_to_csv(aoi_tif, grid)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def build_stack_cdf(input_numerical_folder, input_categorical_folder, output_folder, aoi_shapefile, working_crs, x_res, y_res, output_format='csv', ensure_tif_reprojection=True):\n",
    "    \"\"\"\n",
    "    DataWrangle main function. Creates a hypercube csv using NetCDF as internal mechanism for enhanced performances.\n",
    "    Outputs a .csv of the HyperCube.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_numerical_folder: str\n",
    "        directory containing numerical layers\n",
    "    input_categorical_folder: str\n",
    "        directory containing categorical layers\n",
    "    output_folder : str\n",
    "        directory for the hypercube output\n",
    "    aoi_shapefile: str\n",
    "        path to a shapefile that has a polygon representing the area of interest\n",
    "    working_crs: str\n",
    "        working CRS, per example 'epsg:26921'\n",
    "    x_res: int\n",
    "        x_resolution\n",
    "    y_res: int\n",
    "        y_resolution\n",
    "    output_format : str, default='csv'\n",
    "        Output format for the cube. Available options are 'csv' or 'netCDF'\n",
    "    ensure_tif_reprojection : bool, default = True\n",
    "        Will automatically perform gdal.warp on each of the individual input files to fix unexpected behaviors when it comes to build a VRT. Performance will be decreased but we suggest to keep it this way.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Examples\n",
    "    -------\n",
    "        Build a HyperCube (csv).\n",
    "        >>> datawrangle.build_stack_cdf(input_numerical_folder='input_layers_numerical/', input_categorical_folder='input_layers_categorical/', output_folder='output/', aoi_shapefile='AOI/AOI_2021.shp', working_crs='epsg:31981', x_res=25, y_res=25)\n",
    "    \"\"\"\n",
    "    warnings.warn(\n",
    "        \"This function is deprecated, Please use cube_vrt if this function throws NetCDF errors\", ResourceWarning)\n",
    "    print(\"Processing...\")\n",
    "    # QAQC the output format\n",
    "    formats = ['csv', 'netcdf']\n",
    "    output_format = output_format.lower()\n",
    "    if output_format not in formats:\n",
    "        raise ValueError(\n",
    "            \"Output format you specified is not supported. Supported types : 'csv', 'netcdf'. Processing aborted.\")\n",
    "\n",
    "    # QAQC THE AOI\n",
    "    if(os.path.isfile(aoi_shapefile)):\n",
    "        aoi = gpd.read_file(aoi_shapefile)\n",
    "        if(len(aoi.geom_type) == 0):\n",
    "            print(\"Your AOI shapefile is empty. Processing aborted.\")\n",
    "            return\n",
    "        else:\n",
    "            for t in aoi.geom_type:\n",
    "                if t != 'Polygon':\n",
    "                    print(\n",
    "                        'There is a geometry that is not a polygon in your AOI. Go in QGIS and fix this. Processing aborted.')\n",
    "                    return\n",
    "\n",
    "            for v in aoi['geometry'].is_valid:\n",
    "                if v == False:\n",
    "                    print(\n",
    "                        'There is an invalid geometry in your polygon (self-crossing itself, per example). Go in QGIS and fix this. Processing aborted.')\n",
    "                    return\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The path of the AOI shapefile you provided is wrong. Please double check. Processing aborted.\")\n",
    "\n",
    "    # QAQC if to see if folder provided do exists and are not empty\n",
    "    numeric_files = glob.glob(os.path.join(input_numerical_folder, \"*.tif\"))\n",
    "    if (len(numeric_files) == 0):\n",
    "        raise ValueError(\n",
    "            \"The input NUMERICAL folder path you provided do not contain any geotiffs. Processing aborted.\")\n",
    "\n",
    "    cat_files = glob.glob(os.path.join(input_categorical_folder, \"*.tif\"))\n",
    "    if (len(cat_files) == 0):\n",
    "        raise ValueError(\n",
    "            \"The input CATEGORICAL folder path you provided do not contain any geotiffs. Processing aborted.\")\n",
    "\n",
    "    if(ensure_tif_reprojection == True):\n",
    "        print(\"Reprojecting your numerical files\")\n",
    "        output_folder_numerical_warped = os.path.join(\n",
    "            output_folder, 'datawrangle_reprojected_numerical/')\n",
    "        _reproject_tif_folder(\n",
    "            input_numerical_folder, output_folder_numerical_warped, working_crs, x_res, y_res)\n",
    "        numeric_files = glob.glob(os.path.join(\n",
    "            output_folder_numerical_warped, '*.tif'))\n",
    "\n",
    "        print(\"Reprojecting your categorical files\")\n",
    "        output_folder_categorical_warped = os.path.join(\n",
    "            output_folder, 'datawrangle_reprojected_categorical/')\n",
    "        _reproject_tif_folder(\n",
    "            input_categorical_folder, output_folder_categorical_warped, working_crs, x_res, y_res)\n",
    "        cat_files = glob.glob(os.path.join(\n",
    "            output_folder_categorical_warped, '*.tif'))\n",
    "\n",
    "    print(\"Cubing numerical files...\")\n",
    "    _to_netcdf(numeric_files, os.path.join('temp_numeric_cube.nc'), working_aoi_polygon=aoi_shapefile, working_crs=working_crs,\n",
    "               x_res=x_res, y_res=y_res, resampling_method='cubic', output_type=gdal.GDT_Unknown)\n",
    "    _rename_bands(numeric_files, os.path.join('temp_numeric_cube.nc'))\n",
    "    print(\"Partial numerical cube on {} layers done\".format(len(numeric_files)))\n",
    "\n",
    "    _to_netcdf(cat_files, os.path.join('temp_cat_cube.nc'), working_aoi_polygon=aoi_shapefile, working_crs=working_crs,\n",
    "               x_res=x_res, y_res=y_res, resampling_method='near', output_type=gdal.GDT_Int16)\n",
    "    _rename_bands(cat_files, os.path.join('temp_cat_cube.nc'))\n",
    "    print(\"Partial categorical cube on {} layers done\".format(len(cat_files)))\n",
    "    print(\"merging categorical and numerical features into one...\")\n",
    "\n",
    "    _merge_cubes(os.path.join('renamed_temp_numeric_cube.nc'), os.path.join(\n",
    "        'renamed_temp_cat_cube.nc'), output_folder, output_format)\n",
    "    print(\"Cube creation completed. Cleaning unnecessary files from the disk\")\n",
    "\n",
    "    temp_ncs = [os.path.join('temp_cat_cube.nc'), os.path.join('temp_numeric_cube.nc'), os.path.join(\n",
    "        'renamed_temp_numeric_cube.nc'), os.path.join('renamed_temp_cat_cube.nc')]\n",
    "    for f in temp_ncs:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_netcdf(input_geotiffs, output_nc, working_aoi_polygon, working_crs, x_res, y_res, resampling_method,\n",
    "               output_type, nodata=-99999):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_type: GDAL outputtype used for experientation\n",
    "    input_geotiffs: isolate dinput directory containing only numerical or categorical dataset\n",
    "    output_nc: Output directory\n",
    "    working_aoi_polygon: working area of interest\n",
    "    working_crs: working CRS\n",
    "    x_res: X resolution\n",
    "    y_res: y resolution\n",
    "    resampling_method : resampling method 'near' for categorical dataset and 'cubic' for numerical\n",
    "    nodata: -99999 represents all the null values in the dataset\n",
    "    Returns\n",
    "    -------\n",
    "    individual processed files  with .nc extension in your selected directory\n",
    "    \"\"\"\n",
    "\n",
    "    buildvrt_options = gdal.BuildVRTOptions(xRes=x_res, yRes=y_res, targetAlignedPixels=True, outputSRS=working_crs,\n",
    "                                            VRTNodata=nodata, resampleAlg=resampling_method, separate=True)\n",
    "    one_vrt = gdal.BuildVRT(\n",
    "        destName=\"\", srcDSOrSrcDSTab=input_geotiffs, options=buildvrt_options)\n",
    "    # We Warp it for alignement, resampling, correct CRS and cropping.\n",
    "    warp_options = gdal.WarpOptions(format='NetCDF', xRes=x_res, yRes=y_res, targetAlignedPixels=True,\n",
    "                                    dstSRS=working_crs,\n",
    "                                    cutlineDSName=working_aoi_polygon, resampleAlg=resampling_method,\n",
    "                                    srcNodata=nodata,\n",
    "                                    dstNodata=nodata,\n",
    "                                    outputType=output_type)\n",
    "    warp_output_vrt = gdal.Warp(\n",
    "        srcDSOrSrcDSTab=one_vrt, destNameOrDestDS=output_nc, options=warp_options)\n",
    "\n",
    "    if(warp_output_vrt == None):\n",
    "        print(\"output of gdal.warp() is None. Possible causes are corrupted input files or inputs files not contained in AOI. Processing aborded.\")\n",
    "\n",
    "        return\n",
    "    warp_output_vrt = None\n",
    "    output_nc = None\n",
    "    return\n",
    "\n",
    "\n",
    "def _rename_bands(input_files, nc_file):\n",
    "    # PH's comment : Jan 28 2022. This function could be removed easily and performances would increase drastically also. It is just a matter of finding how to rename \"dimensions\" in rioxarray which I did not knew the week I built datawrangle.\n",
    "\n",
    "    renames = {}\n",
    "    for index, f in enumerate(input_files):\n",
    "        filename, directory, extension = utilities.Path_Info(f)\n",
    "        band_name = 'Band' + str(index + 1)\n",
    "        renames[band_name] = filename\n",
    "    # rename the output_nc step\n",
    "    ds = xarray.open_dataset(nc_file)\n",
    "    x = ds.rename(renames)\n",
    "\n",
    "    x.to_netcdf(os.path.join('renamed_' + nc_file))\n",
    "    ds.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _merge_cubes(a, b, output_folder, output_format):\n",
    "    print(\"If merging looks to take forever, restart your kernel and delete the temp files.\")\n",
    "    cube = xarray.open_mfdataset([a, b], parallel=True)\n",
    "    cube = cube.drop_vars(['transverse_mercator'])\n",
    "    print('Merging completed')\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if(output_format == 'csv'):\n",
    "        print(\"Writing ouput CSV, this is the longest step, it may take a while...\")\n",
    "        out_name = os.path.join(output_folder, 'Hypercube_' + now + '.csv')\n",
    "        df = cube.to_dataframe().reset_index()\n",
    "        #     df.drop(columns=['transverse_mercator'], inplace=True)\n",
    "        df.set_index(['x', 'y'], inplace=True)\n",
    "        df = df.dropna(axis=0, how='all')\n",
    "        df.to_csv(out_name, index=True, chunksize=10000)\n",
    "    elif(output_format == 'netcdf'):\n",
    "        print(\"Writing output NetCDF file (nc)...\")\n",
    "        out_name = os.path.join(\n",
    "            output_folder, 'Netcdf_Hypercube_' + now + '.nc')\n",
    "        cube.to_netcdf(out_name)\n",
    "\n",
    "    cube.close()\n",
    "    print(\"Cube built with success.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def reconstruct_geotiffs_from_cube(input_cube, output_folder, crs, x_field='x', y_field='y', xRes=25, yRes=25):\n",
    "    \"\"\"\n",
    "    QAQC function to create rasters for each column of the HyperCube. The goal is that both the geologist and data scientist could have a look at the resampled, realigned geotiff and confirm the cube is well in shaped and ML work can start.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_cube : str\n",
    "        path to the input csv of the HyperCube\n",
    "    output_folder : str\n",
    "        directory where to save new output folders\n",
    "    crs : str\n",
    "        crs of the project, example 'epsg:26921'\n",
    "    x_field : str, default='x'\n",
    "        name of the x coordinate column\n",
    "    y_field : str, default='y'\n",
    "        name of the y coordinate column\n",
    "    xRes : int, default=25\n",
    "        x resolution\n",
    "    yRes : int, default=25\n",
    "        y resolution\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Examples\n",
    "    --------\n",
    "        Rasterize each column of the cube back to its original geotiff format.\n",
    "        >>> datawrangle.reconstruct_geotiffs_from_cube(input_cube='Netcdf_Hypercube_10-22-2021_1621.csv', output_folder='QAQC_outputs/', crs='epsg:26921', x_field='x', y_field='y', xRes=25, yRes=25)\n",
    "    \"\"\"\n",
    "    # open the dataframe, read only the columns\n",
    "    df = pd.read_csv(input_cube, nrows=2)\n",
    "    cols = df.columns\n",
    "\n",
    "    # get all columns\n",
    "    for col in cols:\n",
    "        if (col != x_field and col != y_field):\n",
    "            print(\"Rebuilding column \" + col)\n",
    "            new_name = 'Rebuilt_from_cube_' + col + '.tif'\n",
    "            output = output_folder + new_name\n",
    "            # utilities.csv_to_raster(input_cube, output,crs, x_field, y_field, col, xRes, yRes)\n",
    "            utilities.csv_to_raster(\n",
    "                input_cube, output, crs, x_field, y_field, col, xRes, yRes)\n",
    "\n",
    "    print('Cube reconstructed. See in ' + output_folder)\n",
    "    return None\n",
    "\n",
    "def get_bands(file_list):\n",
    "    bands = []\n",
    "    for e in file_list:\n",
    "        filename, ext, directory = utilities.Path_Info(e)\n",
    "        bands.append(filename)\n",
    "    return bands\n",
    "\n",
    "def set_band_descriptions(filepath, bands):\n",
    "    \"\"\"\n",
    "    filepath: path/virtual path/uri to raster\n",
    "    bands:    ((band, description), (band, description),...)\n",
    "    \"\"\"\n",
    "    bands_numbers = np.arange(1, len(bands) + 1, 1).tolist()\n",
    "    bands = zip(bands_numbers, bands)\n",
    "    ds = gdal.Open(filepath, gdal.GA_Update)\n",
    "    for band, desc in bands:\n",
    "        rb = ds.GetRasterBand(band)\n",
    "        rb.SetDescription(desc)\n",
    "    del ds\n",
    "\n",
    "def cube_vrt(input_folders_list, \n",
    "             output_folder, \n",
    "             x_res, \n",
    "             y_res, \n",
    "             working_crs, \n",
    "             working_aoi_polygon, \n",
    "             save_vrt=True, \n",
    "             save_csv=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_folders_list : list\n",
    "        list of list with all folders containing input data\n",
    "    output_folder : str\n",
    "        Given output folder name will be created containing the cube data.\n",
    "    x_res: int\n",
    "        x resolution\n",
    "    y_res: int\n",
    "        y resolution\n",
    "    working_crs : str\n",
    "        working CRS, per example 'epsg:26921'\n",
    "    working_aoi_polygon: str\n",
    "        path to a shapefile that has a polygon representing the area of interest\n",
    "    save_vrt : Boolean\n",
    "        default : True\n",
    "        Saves vrt format of the cube in output folder\n",
    "    save_csv : Boolean\n",
    "        default : True\n",
    "        Saves csv format of the cube in output folder\n",
    "    Returns\n",
    "    -------\n",
    "    cube : pandas.DataFrame\n",
    "        resultant cube\n",
    "    Examples\n",
    "    --------\n",
    "    # cube\n",
    "    crs = 'epsg:32733'\n",
    "    x_res = 10\n",
    "    y_res = 10\n",
    "    output_folder = 'project_outputs/'\n",
    "    pred_aoi = 'aoi/Prediction AOI.shp'\n",
    "    geofez_layers = 'prediction_features/geofez/'\n",
    "    haralick_layers = 'prediction_features/unidirectional_w_5_d_1/'\n",
    "    stat_layers = 'prediction_features/stat_features_window_kernel_5_5/'\n",
    "    lbp_layers = 'prediction_features/lbp/'\n",
    "    label_layers = 'prediction_features/labels/'\n",
    "    input_directories = [geofez_layers,haralick_layers,stat_layers,lbp_layers,label_layers]\n",
    "    datawrangle.cube_vrt(input_directories, output_folder, x_res, y_res, crs, pred_aoi, \n",
    "    save_vrt=True, save_csv=True)\n",
    "    \"\"\"\n",
    "        \n",
    "    all_files = []\n",
    "    bands = [] \n",
    "    columns = {}\n",
    "    coord = {'coordinates' : ['x', 'y']} \n",
    "    columns.update(coord)\n",
    "    input_name_list = []\n",
    "    \n",
    "    for f in input_folders_list:\n",
    "        \n",
    "        temp = glob.glob(os.path.join(f, \"*.tif\"))\n",
    "        \n",
    "        for j in temp:\n",
    "            input_name = os.path.basename(j).split('.')[0]\n",
    "            input_name_list.append(input_name)\n",
    "        \n",
    "        if(len(temp) == 0):\n",
    "            raise ValueError(\n",
    "                \"The input folder path %f you provided do not contain any geotiffs. Processing aborted.\" % f)\n",
    "        all_files.extend(temp)\n",
    "        bands.extend(get_bands(temp))\n",
    "        features = {'features' : input_name_list}\n",
    "        \n",
    "    columns.update(features)\n",
    "        \n",
    "    parent = Path(input_folders_list[0]).parent\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "\n",
    "    nodata = -9999\n",
    "    #print(\"There are %s numerical layers and %s categorical layers \"% (str(len(numeric_files)), str(len(cat_files))))\n",
    "\n",
    "    buildvrt_options1 = gdal.BuildVRTOptions(xRes=x_res, yRes=y_res, targetAlignedPixels=True, outputSRS=working_crs,\n",
    "                                             VRTNodata=nodata, resampleAlg='near', separate=True)\n",
    "\n",
    "    combined = os.path.join(parent, \"combined_cat_numeric_vrt_\"+now+\".vrt\")\n",
    "    numerical_vrt = gdal.BuildVRT(\n",
    "        destName=combined, srcDSOrSrcDSTab=all_files, options=buildvrt_options1)\n",
    "    numerical_vrt = None\n",
    "\n",
    "    # change band1 band2 band3 by the real variable name\n",
    "    #bands = get_bands(numeric_files) + get_bands(cat_files)\n",
    "    set_band_descriptions(combined, bands)\n",
    "\n",
    "    warp_options = gdal.WarpOptions(format='vrt', xRes=x_res, yRes=y_res, targetAlignedPixels=True,\n",
    "                                    dstSRS=working_crs,\n",
    "                                    cutlineDSName=working_aoi_polygon, resampleAlg='near',\n",
    "                                    cropToCutline=True,\n",
    "                                    outputType=gdal.GDT_Float32,\n",
    "                                    dstNodata=nodata, copyMetadata=True\n",
    "                                    )\n",
    "\n",
    "    output_vrt = os.path.join(parent, 'hypercube_' + now + '.vrt')\n",
    "    warp_output_vrt = gdal.Warp(\n",
    "        srcDSOrSrcDSTab=combined, destNameOrDestDS=output_vrt, options=warp_options)\n",
    "    warp_output_vrt = None\n",
    "\n",
    "    print('Converting vrt to dataframe... may take a while')\n",
    "        \n",
    "    cube = read_vrt_to_df(output_vrt)\n",
    "    \n",
    "    print('Now writing columns in json')\n",
    "    now = utilities.actual_time_for_file_name()    \n",
    "    output_json_name = os.path.join(output_folder, 'columns_' + now + '.json')\n",
    "    with open(os.path.join(output_json_name), 'w') as fp:\n",
    "        json.dump(columns, fp)      \n",
    "       \n",
    "    if(save_csv == True):\n",
    "        print('Now writing csv ....... may take a while')\n",
    "        output_cube_name = os.path.join(output_folder, os.path.basename(output_vrt).split('.')[0] + '.csv')\n",
    "        cube.to_csv(output_cube_name, index=False)\n",
    "    else:\n",
    "        print(\"Not saving the cube to csv.\")\n",
    "              \n",
    "    if(save_vrt == False):\n",
    "        os.remove(combined)\n",
    "        os.remove(output_vrt)\n",
    "    else:\n",
    "        print(\"Your hypercube was also saved under .vrt format (two files necessary) in the input data folder : %s \" % parent)\n",
    "\n",
    "    print(\"Cube Creation Completed. \")\n",
    "    return cube, columns\n",
    "\n",
    "def add_features(\n",
    "    current_dataframe,\n",
    "    input_directories,\n",
    "    output_folder,\n",
    "    x_res,\n",
    "    y_res,\n",
    "    crs,\n",
    "    aoi_shapefile,\n",
    "    save_vrt=False,\n",
    "    save_csv=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Add extra features to existing data cube\n",
    "    Example:\n",
    "    trial_name = 'RawBaseline_Amaruq_May29'\n",
    "    input_directories = ['targets/']\n",
    "    output_folder = os.path.join(trial_name, 'outputs/')\n",
    "    x_res = 25\n",
    "    y_res = 25\n",
    "    crs = 'epsg:26914'\n",
    "    aoi_shapefile_amaruq = 'AOI_Amaruq/Amaruq_AOI.shp'\n",
    "    df_combined, columns = datawrangle.add_features(\n",
    "            df_current,    \n",
    "            input_directories,\n",
    "            output_folder, \n",
    "            x_res, \n",
    "            y_res,\n",
    "            crs, \n",
    "            aoi_shapefile_amaruq, \n",
    "            save_vrt=True,\n",
    "            save_csv=False\n",
    "            )\n",
    "    \"\"\"\n",
    "    df_addition, _ = cube_vrt(\n",
    "        input_directories,\n",
    "        output_folder, \n",
    "        x_res,\n",
    "        y_res,\n",
    "        crs, \n",
    "        aoi_shapefile,\n",
    "        save_vrt=save_vrt, \n",
    "        save_csv=save_csv\n",
    "        )\n",
    "\n",
    "    columns = {}\n",
    "    coord = {'coordinates' : ['x', 'y']} \n",
    "    columns.update(coord)\n",
    "\n",
    "    current_dataframe = pd.merge(current_dataframe, df_addition, on=(columns['coordinates']))\n",
    "\n",
    "    feature_list = current_dataframe.columns[(current_dataframe.columns != 'x') & (current_dataframe.columns != 'y')].to_list()\n",
    "    features = {'features' : feature_list}\n",
    "    columns.update(features)\n",
    "    \n",
    "    print('Now writing columns in json')\n",
    "    now = utilities.actual_time_for_file_name()\n",
    "    output_json_name = os.path.join(output_folder, 'columns_' + now + '.json')\n",
    "    with open(os.path.join(output_json_name), 'w') as fp:\n",
    "        json.dump(columns, fp)  \n",
    "        \n",
    "    if(save_csv == True):\n",
    "        now = utilities.actual_time_for_file_name()\n",
    "        output_cube = os.path.join(output_folder, 'hypercube_' + now + '.csv')\n",
    "        print('Now writing csv ....... may take a while')\n",
    "        current_dataframe.to_csv(output_cube, index=False)\n",
    "        print('Done!')\n",
    "    else:\n",
    "        print(\"Not saving the cube to csv.\")\n",
    "\n",
    "    return current_dataframe, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f64ddcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479292.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479297.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479302.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479307.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479312.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y  Target\n",
       "0  479292.5  5760117.5     NaN\n",
       "1  479297.5  5760117.5     NaN\n",
       "2  479302.5  5760117.5     NaN\n",
       "3  479307.5  5760117.5     NaN\n",
       "4  479312.5  5760117.5     NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_dask = dd.read_csv(os.path.join('Baseline_Model/outputs/', 'Targets_Merged-01.csv'))\n",
    "df_target_dask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be450b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479292.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479297.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479302.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479307.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479312.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y  Target\n",
       "0  479292.5  5760117.5     NaN\n",
       "1  479297.5  5760117.5     NaN\n",
       "2  479302.5  5760117.5     NaN\n",
       "3  479307.5  5760117.5     NaN\n",
       "4  479312.5  5760117.5     NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_pandas = pd.read_csv(os.path.join('Baseline_Model/outputs/', 'Targets_Merged-01.csv'))\n",
    "df_target_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0084afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "import numpy as np\n",
    "from GeoDS import utilities\n",
    "import xarray\n",
    "\n",
    "class HyperCube:\n",
    "    \"\"\"\n",
    "    Class with methods to clean, access and control data from a hypercube created by DataWrangler\n",
    "    \"\"\"\n",
    "    def __init__(self, input_data, x_field, y_field, crs, columns_dict=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Hypercube object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data : pandas dataframe or dask dataframe or str\n",
    "            dataframe or file path to a csv file or a netcdf file\n",
    "        x_field : str\n",
    "            name of the x coordinate field\n",
    "        y_field : str\n",
    "            name of the y coordinate field\n",
    "        z_field : str, OPTIONAL\n",
    "            name of the z coordinate field\n",
    "        crs : str\n",
    "            coordinate system, example : EPSG:26921\n",
    "        columns_dict : dict, optional\n",
    "            dictionary of the columns\n",
    "        \"\"\"\n",
    "\n",
    "        self.crs = crs\n",
    "        if 'z_field' in kwargs: \n",
    "            self.coordinates = [x_field, y_field, kwargs['z_field']]\n",
    "        else:\n",
    "            self.coordinates = [x_field, y_field]\n",
    "\n",
    "        self.x = x_field\n",
    "        self.y = y_field\n",
    "        if 'z_field' in kwargs: self.z = kwargs['z_field']\n",
    "\n",
    "        self.columns_dict = columns_dict\n",
    "        if (self.columns_dict != None):\n",
    "            cols_list = []\n",
    "            for key, value in self.columns_dict.items():\n",
    "                cols_list.extend(value)\n",
    "        else:\n",
    "            cols_list = None\n",
    "\n",
    "        if type(input_data) == str:    \n",
    "            file, extension, directory = utilities.Path_Info(input_data)\n",
    "            if (extension == '.csv'):\n",
    "                self.df = pd.read_csv(input_data, usecols=cols_list)\n",
    "            elif(extension == '.nc'):\n",
    "                xr = xarray.open_dataset(input_data)\n",
    "                self.df = xr.to_dataframe().reset_index()\n",
    "\n",
    "                no_coords = self.df.columns.values.tolist().copy()\n",
    "                no_coords.remove(x_field)\n",
    "                no_coords.remove(y_field)\n",
    "                if 'z_field' in kwargs: no_coords.remove(kwargs['z_field'])\n",
    "\n",
    "                self.df.dropna(axis=0, how='all', subset=no_coords, inplace=True)\n",
    "\n",
    "            else:\n",
    "                print('Error. We do not support the following format : ' + extension)\n",
    "                        \n",
    "        elif type(input_data) == dask.dataframe.core.DataFrame or type(input_data) == pd.core.frame.DataFrame:\n",
    "            self.df = input_data.copy()\n",
    "                    \n",
    "        return  \n",
    "\n",
    "    def set_as_categorical(self, subsets):\n",
    "        for e in subsets:\n",
    "            fields = self.columns_dict[e]\n",
    "            for f in fields:\n",
    "                self.df[f] = self.df[f].astype(str)\n",
    "        return\n",
    "\n",
    "    def replace_null(self, null_values, destination_null = np.nan):\n",
    "        \"\"\"\n",
    "        Perform a df.replace on all the cube.\n",
    "        Parameters\n",
    "        ----------\n",
    "        null_values : str, regex, list, dict, Series, int, float, or None\n",
    "            values to replace (like df.replace)\n",
    "        destination_null : scalar, dict, list, str, regex, default np.nan\n",
    "            value argument of df.replace what you want to replace with. By default we use np.nan\n",
    "        \"\"\"\n",
    "        self.df.replace(to_replace=null_values, value=destination_null, inplace=True)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def GetCatFeaturesIndexes(self, dataframe):\n",
    "        \"\"\"\n",
    "        Get a list of indices of the columns with categorical features. Assumption is that you previously assigned a 'categories' key in the cube.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pandas.DataFrame\n",
    "            The input DataFrame. This function should be called on cube.getRowsWithLabelOnly() return.\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of index\n",
    "        \"\"\"\n",
    "        cat_col = self.columns_dict['categories']\n",
    "        categorical_features_indices = [dataframe.columns.get_loc(c) for c in cat_col]\n",
    "        return categorical_features_indices\n",
    "\n",
    "\n",
    "    def CategoryToOriginalString(self, category_column, input_csv, key_column='0', value_column='Unnamed: 0', verbose=False):\n",
    "        \"\"\"\n",
    "        Switch back the categories to their original strings. A geology raster processed through FeatureEngineer would be rasterized as 0, 1, 2,3, etc.\n",
    "        You want have your cube containing Granite, Gabbro, Rhyolite, etc.. instead.\n",
    "        FeatureEngineer will have saved a csv of the code correspondance. HyperCube.df attribute will be modified in-place.\n",
    "        Parameters\n",
    "        ----------\n",
    "        category_column : str\n",
    "            Name of the column in the cube you want to replace the values.\n",
    "        input_csv : str\n",
    "            Path to the code correspondance csv\n",
    "        key_column : str\n",
    "            key column name in the code correspondance csv\n",
    "        value_column : str\n",
    "            value column name in the code correspondance csv\n",
    "        verbose : bool, default = False\n",
    "            argument set to False by default. Set it to True if you want to have the correspondance printed out in the notebook/terminal.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        correspondance = pd.read_csv(input_csv)\n",
    "        corres = dict(zip(correspondance[key_column], correspondance[value_column]))\n",
    "\n",
    "\n",
    "        self.df.replace({category_column: corres}, inplace=True)\n",
    "        if(verbose==True):\n",
    "            print(corres)\n",
    "            print(\"Category Replaced\")\n",
    "        return None\n",
    "\n",
    "    def apply_mask(self, mask_key):\n",
    "        \"\"\"\n",
    "        Given you have a mask column in your cube (0 and 1), filters out data : 0 will be kept and 1 will be removed.\n",
    "        Affects the df attribute inplace.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mask_key : str\n",
    "            key of the column in your hypercube to use as the filter\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            Affects the DataFrame inplace.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df[mask_key].fillna(0, inplace=True)\n",
    "        self.df = self.df.loc[~self.df[mask_key].astype('bool'), :]\n",
    "        return\n",
    "\n",
    "\n",
    "    def subset(self, keys):\n",
    "        \"\"\"\n",
    "        Function to get a portion of a dataframe based on the keys from columns_dict attribute.\n",
    "        Parameters\n",
    "        ----------\n",
    "        keys : list\n",
    "            keys of the column dictionary (columns_dict attribute) : example ['geophysics', 'categories']\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            a DataFrame with only columns from the subsets asked.\n",
    "        \"\"\"\n",
    "\n",
    "        #Could also call __subsetCols\n",
    "        cols = []\n",
    "        for k in keys:\n",
    "            for i in self.columns_dict[k]:\n",
    "                cols.append(i)\n",
    "\n",
    "        return self.df.loc[:,cols]\n",
    "\n",
    "    def __subsetCols(self, keys):\n",
    "        \"\"\"\n",
    "        PRIVATE function to get only the columns names from given subset. Example, ['geophysics', 'categories'] would return ['TMI', '1st_VD', '2nd_VD', 'geologic_map', ...]\n",
    "        Parameters\n",
    "        ----------\n",
    "        keys : list\n",
    "            subsets (keys) to look for in the columns_dict attribute\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of columns names from given subset\n",
    "        \"\"\"\n",
    "        cols = []\n",
    "        for k in keys:\n",
    "            for i in self.columns_dict[k]:\n",
    "                cols.append(i)\n",
    "        return cols\n",
    "\n",
    "    def to_object(self, columns):\n",
    "        \"\"\"\n",
    "        Allows to convert one or many columns to dtype object (string). Lets say you have a layer of 1,2,3,4,5.. clusters from remote sensing. You want to have this column flagged as 'object' type so that\n",
    "        columns transformers (and CatBoost) can recognize this.\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns : str, list\n",
    "            name of the column, or list of many columns that needs to be converted\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if(isinstance(columns, str)):\n",
    "            self.df[columns] = self.df[columns].astype(str)\n",
    "        if(isinstance(columns, list)):\n",
    "            for c in columns:\n",
    "                self.df[c] = self.df[c].astype(str)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Print df.info() summary from the cube's data and make sure all the columns are displayed.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        cols = len(self.df.columns.values)\n",
    "        # pd.set_option('display.max_columns', 160)\n",
    "        return self.df.info(max_cols=cols, show_counts=True)\n",
    "\n",
    "    def show_nan_percentage(self):\n",
    "        \"\"\"\n",
    "        Show the percentage of Nan values\n",
    "        \"\"\"\n",
    "\n",
    "        for column in self.df.columns:\n",
    "            null_rate = self.df[column].isna().sum() / len(self.df) * 100\n",
    "            if null_rate > 0:\n",
    "                print(\"{}'s null rate :{}%\".format(\n",
    "                    column, round(null_rate, 5)))\n",
    "\n",
    "    def dropna(self, subset, how=\"any\", inplace=True):\n",
    "        \"\"\"\n",
    "        Perform a DataFrame.dropna function on the columns of a given subset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : str\n",
    "            subset (geophysics, remote sensing, category... etc.)\n",
    "        how :str, default='any'\n",
    "            'any' or 'all' default is 'any'\n",
    "        inplace : bool, default=True\n",
    "            True or False\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame or None\n",
    "            DataFrame with NA entries dropped from it or None if inplace=True.\n",
    "        \"\"\"\n",
    "        if(inplace==True):\n",
    "            self.df.dropna(subset=self.columns_dict[subset], how=how, inplace=inplace)\n",
    "            return None\n",
    "        else:\n",
    "            return self.df.dropna(subset=self.columns_dict[subset], how=how, inplace=inplace)\n",
    "\n",
    "    def SetColumnsDict(self, columns):\n",
    "        \"\"\"\n",
    "        Not all implemented yet.\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns :\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        print(\"NOT IMPLEMENTED YET\")\n",
    "        existing_cols = self.df.columns.values.tolist()\n",
    "        if type(columns) is dict:\n",
    "            # Ensure that the given columns are in the dataframe\n",
    "            for key in columns:\n",
    "                for e in columns[key]:\n",
    "                    if (e not in existing_cols):\n",
    "                        print(\"Input column \" + e + \" is not in the input cube.\")\n",
    "                        return\n",
    "\n",
    "            # if we are here, that means columns were correctly input.\n",
    "            self.columns_dict = columns\n",
    "        else:\n",
    "            print(\"Input should be a dictionary.\")\n",
    "            return\n",
    "\n",
    "    def getRowsWithLabelOnly(self, subsets, target, target_type='categorical'):\n",
    "        \"\"\"\n",
    "        Get a X and y subset of the DataFrame with only rows that have a label in a certain target columm.\n",
    "        Parameters\n",
    "        ----------\n",
    "        subsets : list\n",
    "            the subsets you want included in the X data. (each one is a str type)\n",
    "        target : str\n",
    "            name of the target column you want for y\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            X a DataFrame for training data that can be sent to to sklearn.test_train_split()\n",
    "        pandas.DataFrame\n",
    "            Y a DataFrame for training data that can be sent to to sklearn.test_train_split()\n",
    "        \"\"\"\n",
    "        data = self.df.dropna(subset=[target], how='any')\n",
    "\n",
    "        cols = self.__subsetCols(subsets)\n",
    "        X = data.loc[:, cols]\n",
    "        y = data.loc[:, target]\n",
    "\n",
    "        # for i in target:\n",
    "        if target_type == 'categorical': y = y.astype({target: 'int32'})\n",
    "        if target_type == 'numerical': y = y.astype({target: 'float32'})\n",
    "        # y = y.astype({target: 'int32'})\n",
    "        # df.dropna(how=any, inplace=True)\n",
    "        # df = df.astype({col: 'int32'})\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd2c1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479292.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479297.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479302.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479307.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479312.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588400</th>\n",
       "      <td>440642.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588401</th>\n",
       "      <td>440647.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588402</th>\n",
       "      <td>440652.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588403</th>\n",
       "      <td>440657.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588404</th>\n",
       "      <td>440662.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3588405 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x          y  Target\n",
       "0        479292.5  5760117.5     NaN\n",
       "1        479297.5  5760117.5     NaN\n",
       "2        479302.5  5760117.5     NaN\n",
       "3        479307.5  5760117.5     NaN\n",
       "4        479312.5  5760117.5     NaN\n",
       "...           ...        ...     ...\n",
       "3588400  440642.5  5759442.5     NaN\n",
       "3588401  440647.5  5759442.5     NaN\n",
       "3588402  440652.5  5759442.5     NaN\n",
       "3588403  440657.5  5759442.5     NaN\n",
       "3588404  440662.5  5759442.5     NaN\n",
       "\n",
       "[3588405 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_empty = pd.DataFrame({'A' : []})\n",
    "#df_empty.to_csv(os.path.join(output_folder,'df_empty.csv'), index=False)\n",
    "\n",
    "#cube = hypercube.HyperCube(input_data=os.path.join(output_folder,'df_empty.csv'), x_field='x', y_field='y', crs=crs)\n",
    "#cube = HyperCube(input_data=os.path.join(output_folder,'df_empty.csv'), x_field='x', y_field='y', crs=crs)\n",
    "cube = HyperCube(input_data=df_target_pandas, x_field='x', y_field='y', crs=crs)\n",
    "\n",
    "#cube.df = df_target.copy()\n",
    "cube.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05fe0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 2 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                     x        y   Target\n",
       "npartitions=2                           \n",
       "               float64  float64  float64\n",
       "                   ...      ...      ...\n",
       "                   ...      ...      ...\n",
       "Dask Name: read-csv, 2 tasks"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube = HyperCube(input_data=df_target_dask, x_field='x', y_field='y', crs=crs)\n",
    "cube.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fbf817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479292.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479297.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479302.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479307.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479312.5</td>\n",
       "      <td>5760117.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388400</th>\n",
       "      <td>440642.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388401</th>\n",
       "      <td>440647.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388402</th>\n",
       "      <td>440652.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388403</th>\n",
       "      <td>440657.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388404</th>\n",
       "      <td>440662.5</td>\n",
       "      <td>5759442.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3588405 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x          y  Target\n",
       "0       479292.5  5760117.5     NaN\n",
       "1       479297.5  5760117.5     NaN\n",
       "2       479302.5  5760117.5     NaN\n",
       "3       479307.5  5760117.5     NaN\n",
       "4       479312.5  5760117.5     NaN\n",
       "...          ...        ...     ...\n",
       "388400  440642.5  5759442.5     NaN\n",
       "388401  440647.5  5759442.5     NaN\n",
       "388402  440652.5  5759442.5     NaN\n",
       "388403  440657.5  5759442.5     NaN\n",
       "388404  440662.5  5759442.5     NaN\n",
       "\n",
       "[3588405 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube.df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b6d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geods Kernel",
   "language": "python",
   "name": "geods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
